{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- Mess with classifier parameters (once working)\n",
    "- output from classification is data frame with slave_logs (maybe rename that column?) indicating:\n",
    "    - cliwoc_data (unclassified) = 0\n",
    "    - cliwoc_data (no slaves)    = 1\n",
    "    - cliwoc_data (slaves)       = 2\n",
    "    - slave_data                 = 3\n",
    "    - classified as slave log    = 4\n",
    "    - classified as non slave log = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_algorithm = \"Decision Tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import exploringShipLogbooks.wordCount as wc\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "\n",
    "from exploringShipLogbooks.basic_utils import clean_data\n",
    "from exploringShipLogbooks.basic_utils import encode_data_df\n",
    "from exploringShipLogbooks.basic_utils import extract_logbook_data\n",
    "from exploringShipLogbooks.basic_utils import isolate_columns\n",
    "from exploringShipLogbooks.basic_utils import isolate_training_data\n",
    "\n",
    "from exploringShipLogbooks.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load and clean data\n",
    "### Load CLIWOC ship logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emma\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3006: DtypeWarning: Columns (5,6,7,8,11,13,18,19,23,24,25,26,28,29,30,34,35,38,43,44,46,73,77,81,82,84,85,87,88,94,96,97,98,99,111,114,116,119,120,122,124,125,127,129,131,133,135,137,140) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# extract data from zip file\n",
    "cliwoc_data = extract_logbook_data('CLIWOC15.csv')\n",
    "#cliwoc_data = cliwoc_data.loc[50000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoding = preprocessing.LabelEncoder().fit(cliwoc_data['LogbookIdent']).classes_\n",
    "cliwoc_data['LogbookIdent'] = preprocessing.LabelEncoder().fit_transform(cliwoc_data['LogbookIdent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find definite slave data in CLIWOC data set\n",
    "- These logs will be used to test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  464  logs that mention slaves\n"
     ]
    }
   ],
   "source": [
    "# extract logs that mention slaves\n",
    "slave_mask = wc.count_key_words(cliwoc_data, text_columns, slave_words)\n",
    "print('Found ', len(slave_mask[slave_mask==True]), ' logs that mention slaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean CLIWOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  6517  logs that are non-slave ships\n"
     ]
    }
   ],
   "source": [
    "# find indices of ship names that are \"non-slave\" ships before dropping ship name column\n",
    "non_slave_log_locations = isolate_training_data(cliwoc_data, {'ShipName': non_slave_ships})\n",
    "print('Found ', len(non_slave_log_locations[non_slave_log_locations==True]), ' logs that are non-slave ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_data['slave_logs'] = np.zeros(len(cliwoc_data))\n",
    "slave_log_locations = cliwoc_data['LogbookIdent'].isin(list(cliwoc_data['LogbookIdent']\n",
    "                                                            [slave_mask].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cliwoc_data (unclassified) = 0\n",
    "- cliwoc_data (no slaves)    = 1\n",
    "- cliwoc_data (slaves)       = 2\n",
    "- slave_data                 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_data.loc[non_slave_log_locations,'slave_logs'] = 1\n",
    "cliwoc_data.loc[slave_log_locations,'slave_logs'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cliwoc_data = cliwoc_data.sort_values('LogbookIdent', ascending=True)\n",
    "cliwoc_data = cliwoc_data.drop_duplicates('LogbookIdent')\n",
    "cliwoc_data = cliwoc_data.set_index('LogbookIdent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment this if looking at ship names for manual review\n",
    "#desired_columns.append('ShipName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove undesired columns\n",
    "cliwoc_data = isolate_columns(cliwoc_data, desired_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Slave Voyages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = './exploringShipLogbooks/data/tastdb-exp-2010'\n",
    "slave_voyage_logs = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_ind = ~(slave_voyage_logs['yeardep'].isnull())\n",
    "slave_voyage_logs = slave_voyage_logs[year_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_ind = (slave_voyage_logs['yeardep']>cliwoc_data['Year'].min()) & (slave_voyage_logs['yeardep']<cliwoc_data['Year'].max())\n",
    "slave_voyage_logs = slave_voyage_logs[cliwoc_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Slave voyages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slave_voyage_desired_cols = list(slave_voyage_conversions.keys())\n",
    "slave_voyage_logs = isolate_columns(slave_voyage_logs, slave_voyage_desired_cols)\n",
    "\n",
    "slave_voyage_logs.rename(columns=slave_voyage_conversions, inplace=True)\n",
    "#slave_voyage_logs.columns = ['Nationality', 'ShipType', 'VoyageFrom', 'VoyageTo', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slave_voyage_logs['slave_logs'] = 3\n",
    "slave_voyage_indices = range(len(slave_voyage_logs)) + (cliwoc_data.tail(1).index[0]+1)\n",
    "slave_voyage_logs = slave_voyage_logs.set_index(slave_voyage_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([cliwoc_data, slave_voyage_logs])\n",
    "#all_data = cliwoc_data.append(slave_voyage_logs)\n",
    "all_data = clean_data(all_data)\n",
    "\n",
    "# cleanup\n",
    "#del cliwoc_data, slave_voyage_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of fuzzywuzzy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from exploringShipLogbooks.fuzz_replacement import fuzzy_wuzzy_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_test = all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationality\n",
      "ShipType\n",
      "VoyageFrom\n",
      "VoyageTo\n",
      "Year\n",
      "slave_logs\n"
     ]
    }
   ],
   "source": [
    "for col in all_data_test.columns.values:\n",
    "    fuzzy_wuzzy_classification(all_data_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "- Must encode data before separating, otherwise values that do not occur in a subset will be encoded differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                if is_instance(X[col][0], str):\n",
    "                    output[col] = LabelEncoder().fit_transform(output[col])\n",
    "                else:\n",
    "                    output[col] = X[col]\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if classifier_algorithm == \"Decision Tree\":\n",
    "    all_data = MultiColumnLabelEncoder().fit_transform(all_data)\n",
    "elif classifier_algorithm == \"Naive Bayes\":\n",
    "    all_data = encode_data_df(all_data)\n",
    "else:\n",
    "    raise KeyError(\"Please enter a valid classification type (Decision Trees or Naive Bayes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove nan columns from encoding (currently there are no empty cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data['no_data'] = all_data['nan'].apply(lambda x: x.any(), axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop('nan', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract training data, and create list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unclassified_logs = all_data[all_data['slave_logs']==0]\n",
    "#unclassified_logs = unclassified_logs.drop('slave_logs', axis=1)\n",
    "\n",
    "validation_set_1 = all_data[all_data['slave_logs']==2]\n",
    "#validation_set_1 = validation_set_1.drop('slave_logs', axis=1)\n",
    "\n",
    "# reserve first 20% of slave_voyage_logs as validation set\n",
    "validation_set_2_indices = range(slave_voyage_indices.min(),\n",
    "                                 slave_voyage_indices.min() + round(len(slave_voyage_indices)*.2))\n",
    "validation_set_2 = all_data.iloc[validation_set_2_indices]\n",
    "#validation_set_2 = validation_set_2.drop('slave_logs', axis=1)\n",
    "\n",
    "training_logs_pos = all_data.drop(validation_set_2_indices)\n",
    "training_logs_pos = training_logs_pos[training_logs_pos['slave_logs']==3]\n",
    "#training_logs_pos = training_logs_pos.drop('slave_logs', axis=1)\n",
    "\n",
    "# note! This relies on cliwoc data being first in all_data\n",
    "# could make more robust later\n",
    "training_logs_neg = all_data[all_data['slave_logs']==1]\n",
    "#training_logs_neg = training_logs_neg.drop('slave_logs', axis=1)\n",
    "\n",
    "# cleanup\n",
    "#del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- left this code so we can check if there are any null values in each \n",
    "  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def finding_null_values(df):\n",
    "    return df.isnull().sum()[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeat_multiplier = round(len(training_logs_pos)/len(training_logs_neg))\n",
    "\n",
    "# create list of classes for training data (0 is for non-slave, 1 is for slave)\n",
    "# index matches training_data\n",
    "classes = np.zeros(len(training_logs_neg)).repeat(repeat_multiplier)\n",
    "#classes = np.append(classes, np.ones(len(training_logs_pos)))\n",
    "classes = np.append(classes, np.ones(len(training_logs_pos)))\n",
    "\n",
    "# join training data\n",
    "neg_rep = pd.concat([training_logs_neg]*repeat_multiplier)\n",
    "training_data = pd.concat([neg_rep, training_logs_pos], ignore_index = True)\n",
    "\n",
    "# convert to numpy array\n",
    "columns = list(training_data.columns)\n",
    "columns.remove('slave_logs')\n",
    "training_data = training_data.as_matrix(columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fit training data to classifier\n",
    "- **note!** first column of numpy array is index! do not include in classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if classifier_algorithm == \"Decision Tree\":\n",
    "    classifier = MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "    classifier.fit(training_data[::,1::], classes)\n",
    "elif classifier_algorithm == \"Naive Bayes\":\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(training_data[::,1::], classes)\n",
    "else: \n",
    "    raise KeyError(\"Please enter a valid classification type (Decision Trees or Naive Bayes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classifier\n",
    "- check if slave logs from cliwoc data were classified correctly (want mostly classified as 1)\n",
    "- compare first column with slave_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_test(classifier, validation_set, expected_class):\n",
    "    \"\"\"\n",
    "    input classifer object, validation set (data frame), and expected class \n",
    "    of validation set (i.e. 1 or 0). Prints successful classification rate.\n",
    "    \"\"\"\n",
    "    columns = list(validation_set.columns)\n",
    "    columns.remove('slave_logs')\n",
    "    validation_set = validation_set.as_matrix(columns)\n",
    "    predictions = classifier.predict(validation_set[::,1::])\n",
    "    \n",
    "    counts = collections.Counter(predictions)\n",
    "    percent_correct = (counts[expected_class]/(len(predictions))* 100)\n",
    "                       \n",
    "    print('Validation set was classified as', expected_class,\n",
    "          round(percent_correct,2), '% of the time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_class(classifier, data_subset):\n",
    "    \"\"\"\n",
    "    Predict class of data, and append predictions to data frame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # drop old predictions before reclassifying (if they exist)\n",
    "        data_subset = data_subset.drop('predictions', axis = 1)\n",
    "        data_to_classify = data_subset.copy()\n",
    "    except:\n",
    "        data_to_classify = data_subset.copy()\n",
    "        pass\n",
    "    \n",
    "    # convert to numpy and classify\n",
    "    columns = list(data_to_classify.columns)\n",
    "    columns.remove('slave_logs')\n",
    "    data_matrix = data_to_classify.as_matrix(columns)\n",
    "    predictions = classifier.predict(data_matrix[::,1::])\n",
    "    \n",
    "    # revalue slave_log ID column to indicate classification\n",
    "    data_to_classify['slave_logs'] = predictions + 4\n",
    "    \n",
    "    # print statstics\n",
    "    counts = collections.Counter(predictions)\n",
    "    \n",
    "    for key in counts:\n",
    "        percent = (counts[key]/(len(predictions))* 100)\n",
    "        print(round(percent, 2), 'of data was classified as ', key)\n",
    "        \n",
    "    # update slave_log columns\n",
    "\n",
    "    return data_to_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validation data from slave logs data set\n",
      "Validation set was classified as 1 94.55 % of the time\n",
      "Testing validation data from cliwoc data set:\n",
      "Validation set was classified as 1 26.79 % of the time\n"
     ]
    }
   ],
   "source": [
    "print('Testing validation data from slave logs data set')\n",
    "validation_test(classifier, validation_set_2, 1)\n",
    "\n",
    "print('Testing validation data from cliwoc data set:')\n",
    "validation_test(classifier, validation_set_1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.3 of data was classified as  0.0\n",
      "34.7 of data was classified as  1.0\n"
     ]
    }
   ],
   "source": [
    "unclassified_logs = predict_class(classifier, unclassified_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nationality</th>\n",
       "      <th>ShipType</th>\n",
       "      <th>VoyageFrom</th>\n",
       "      <th>VoyageTo</th>\n",
       "      <th>Year</th>\n",
       "      <th>slave_logs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogbookIdent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>175</td>\n",
       "      <td>307</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>175</td>\n",
       "      <td>307</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>38</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>288</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>406</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Nationality  ShipType  VoyageFrom  VoyageTo  Year  slave_logs\n",
       "LogbookIdent                                                               \n",
       "0                       5        83         175       307   123           4\n",
       "1                       5        83         175       307   127           4\n",
       "2                      17        46          38        93   185           5\n",
       "3                      17        82          68       288   187           4\n",
       "9                       5        83         406        36     0           5"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclassified_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try decision trees plotting\n",
    "- doesn't work need to install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# export PDF with decision tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "import os\n",
    "import pydot \n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(new_classifier, out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"test.pdf\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
