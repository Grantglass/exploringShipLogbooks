{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- Mess with classifier parameters (once working)\n",
    "- have variable at start to switch classifier (because different encoding)\n",
    "- output from classification is data frame with slave_logs (maybe rename that column?) indicating:\n",
    "    - cliwoc_data (unclassified) = 0\n",
    "    - cliwoc_data (no slaves)    = 1\n",
    "    - cliwoc_data (slaves)       = 2\n",
    "    - slave_data                 = 3\n",
    "    - classified as slave log    = 4\n",
    "    - classified as non slave log = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import exploringShipLogbooks.wordCount as wc\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from exploringShipLogbooks.basic_utils import clean_data\n",
    "from exploringShipLogbooks.basic_utils import encode_data_df\n",
    "from exploringShipLogbooks.basic_utils import extract_logbook_data\n",
    "from exploringShipLogbooks.basic_utils import isolate_columns\n",
    "from exploringShipLogbooks.basic_utils import isolate_training_data\n",
    "\n",
    "from exploringShipLogbooks.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load and clean data\n",
    "### Load CLIWOC ship logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emma\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3006: DtypeWarning: Columns (5,6,7,8,11,13,18,19,23,24,25,26,28,29,30,34,35,38,43,44,46,73,77,81,82,84,85,87,88,94,96,97,98,99,111,114,116,119,120,122,124,125,127,129,131,133,135,137,140) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# extract data from zip file\n",
    "cliwoc_data = extract_logbook_data('CLIWOC15.csv')\n",
    "#cliwoc_data = cliwoc_data.loc[50000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoding = preprocessing.LabelEncoder().fit(cliwoc_data['LogbookIdent']).classes_\n",
    "cliwoc_data['LogbookIdent'] = preprocessing.LabelEncoder().fit_transform(cliwoc_data['LogbookIdent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find definite slave data in CLIWOC data set\n",
    "- These logs will be used to test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  464  logs that mention slaves\n"
     ]
    }
   ],
   "source": [
    "# extract logs that mention slaves\n",
    "slave_mask = wc.count_key_words(cliwoc_data, text_columns, slave_words)\n",
    "print('Found ', len(slave_mask[slave_mask==True]), ' logs that mention slaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean CLIWOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  6517  logs that are non-slave ships\n"
     ]
    }
   ],
   "source": [
    "# find indices of ship names that are \"non-slave\" ships before dropping ship name column\n",
    "non_slave_log_locations = isolate_training_data(cliwoc_data, {'ShipName': non_slave_ships})\n",
    "print('Found ', len(non_slave_log_locations[non_slave_log_locations==True]), ' logs that are non-slave ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_data['slave_logs'] = np.zeros(len(cliwoc_data))\n",
    "slave_log_locations = cliwoc_data['LogbookIdent'].isin(list(cliwoc_data['LogbookIdent']\n",
    "                                                            [slave_mask].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cliwoc_data (unclassified) = 0\n",
    "- cliwoc_data (no slaves)    = 1\n",
    "- cliwoc_data (slaves)       = 2\n",
    "- slave_data                 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_data.loc[non_slave_log_locations,'slave_logs'] = 1\n",
    "cliwoc_data.loc[slave_log_locations,'slave_logs'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cliwoc_data = cliwoc_data.sort_values('LogbookIdent', ascending=True)\n",
    "cliwoc_data = cliwoc_data.drop_duplicates('LogbookIdent')\n",
    "cliwoc_data = cliwoc_data.set_index('LogbookIdent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment this if looking at ship names for manual review\n",
    "#desired_columns.append('ShipName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove undesired columns\n",
    "cliwoc_data = isolate_columns(cliwoc_data, desired_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Slave Voyages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = './exploringShipLogbooks/data/tastdb-exp-2010'\n",
    "slave_voyage_logs = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_ind = ~(slave_voyage_logs['yeardep'].isnull())\n",
    "slave_voyage_logs = slave_voyage_logs[year_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliwoc_ind = (slave_voyage_logs['yeardep']>cliwoc_data['Year'].min()) & (slave_voyage_logs['yeardep']<cliwoc_data['Year'].max())\n",
    "slave_voyage_logs = slave_voyage_logs[cliwoc_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Slave voyages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slave_voyage_desired_cols = list(slave_voyage_conversions.keys())\n",
    "slave_voyage_logs = isolate_columns(slave_voyage_logs, slave_voyage_desired_cols)\n",
    "\n",
    "slave_voyage_logs.rename(columns=slave_voyage_conversions, inplace=True)\n",
    "#slave_voyage_logs.columns = ['Nationality', 'ShipType', 'VoyageFrom', 'VoyageTo', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slave_voyage_logs['slave_logs'] = 3\n",
    "slave_voyage_indices = range(len(slave_voyage_logs)) + (cliwoc_data.tail(1).index[0]+1)\n",
    "slave_voyage_logs = slave_voyage_logs.set_index(slave_voyage_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([cliwoc_data, slave_voyage_logs])\n",
    "#all_data = cliwoc_data.append(slave_voyage_logs)\n",
    "all_data = clean_data(all_data)\n",
    "\n",
    "# cleanup\n",
    "#del cliwoc_data, slave_voyage_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of fuzzywuzzy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df = pd.DataFrame( {'id':[1, 2, 3, 4, 5, 6], 'name':['dog', 'cat', 'mad cat', 'good dog', 'bad dog', 'chicken']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(name):\n",
    "    matches = df.apply(lambda row: (fuzz.partial_ratio(row['name'], name) >= 85), axis=1)\n",
    "    return [i for i, x in enumerate(matches) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0, 3, 4]\n",
       "1       [1, 2]\n",
       "2       [1, 2]\n",
       "3       [0, 3]\n",
       "4       [0, 4]\n",
       "5          [5]\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda row: func(row['name']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try fuzzywuzzy on subset of one of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(name, column_name):\n",
    "    matches = all_data[0:10].apply(lambda row: (fuzz.partial_ratio(row[column_name], name) >= 85), axis=1)\n",
    "    return [i for i, x in enumerate(matches) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_name = 'Nationality'\n",
    "all_data[0:10].apply(lambda row: func(row[column_name], column_name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "- Must encode data before separating, otherwise values that do not occur in a subset will be encoded differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                if is_instance(X[col][0], str):\n",
    "                    output[col] = LabelEncoder().fit_transform(output[col])\n",
    "                else:\n",
    "                    output[col] = X[col]\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = MultiColumnLabelEncoder().fit_transform(all_data)\n",
    "\n",
    "type(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temporarily removed. use decision tree classifier with the label encoded data!\n",
    "#all_data = encode_data_df(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove nan columns from encoding (currently there are no empty cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data['no_data'] = all_data['nan'].apply(lambda x: x.any(), axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop('nan', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract training data, and create list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unclassified_logs = all_data[all_data['slave_logs']==0]\n",
    "unclassified_logs = unclassified_logs.drop('slave_logs', axis=1)\n",
    "\n",
    "validation_set_1 = all_data[all_data['slave_logs']==2]\n",
    "validation_set_1 = validation_set_1.drop('slave_logs', axis=1)\n",
    "\n",
    "# reserve first 20% of slave_voyage_logs as validation set\n",
    "validation_set_2_indices = range(slave_voyage_indices.min(),\n",
    "                                 slave_voyage_indices.min() + round(len(slave_voyage_indices)*.2))\n",
    "validation_set_2 = all_data.iloc[validation_set_2_indices]\n",
    "validation_set_2 = validation_set_2.drop('slave_logs', axis=1)\n",
    "\n",
    "training_logs_pos = all_data.drop(validation_set_2_indices)\n",
    "training_logs_pos = training_logs_pos[training_logs_pos['slave_logs']==3]\n",
    "training_logs_pos = training_logs_pos.drop('slave_logs', axis=1)\n",
    "\n",
    "# note! This relies on cliwoc data being first in all_data\n",
    "# could make more robust later\n",
    "training_logs_neg = all_data[all_data['slave_logs']==1]\n",
    "training_logs_neg = training_logs_neg.drop('slave_logs', axis=1)\n",
    "\n",
    "# cleanup\n",
    "#del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- left this code so we can check if there are any null values in each \n",
    "  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def finding_null_values(df):\n",
    "    return df.isnull().sum()[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeat_multiplier = round(len(training_logs_pos)/len(training_logs_neg))\n",
    "\n",
    "# create list of classes for training data (0 is for non-slave, 1 is for slave)\n",
    "# index matches training_data\n",
    "classes = np.zeros(len(training_logs_neg)).repeat(repeat_multiplier)\n",
    "#classes = np.append(classes, np.ones(len(training_logs_pos)))\n",
    "classes = np.append(classes, np.ones(len(training_logs_pos)))\n",
    "\n",
    "# join training data\n",
    "neg_rep = pd.concat([training_logs_neg]*repeat_multiplier)\n",
    "training_data = pd.concat([neg_rep, training_logs_pos], ignore_index = True)\n",
    "\n",
    "# convert to numpy array\n",
    "training_data = training_data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fit training data to classifier\n",
    "- **note!** first column of numpy array is index! do not include in classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "classifier.fit(training_data[::,1::], classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classifier\n",
    "- check if slave logs from cliwoc data were classified correctly (want mostly classified as 1)\n",
    "- compare first column with slave_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_test(classifier, validation_set, expected_class):\n",
    "    \"\"\"\n",
    "    input classifer object, validation set (data frame), and expected class \n",
    "    of validation set (i.e. 1 or 0). Prints successful classification rate.\n",
    "    \"\"\"\n",
    "    validation_set = validation_set.as_matrix()\n",
    "    predictions = classifier.predict(validation_set[::,1::])\n",
    "    \n",
    "    counts = collections.Counter(predictions)\n",
    "    percent_correct = (counts[expected_class]/(len(predictions))* 100)\n",
    "                       \n",
    "    print('Validation set was classified as', expected_class,\n",
    "          round(percent_correct,2), '% of the time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_class(classifier, data_subset):\n",
    "    \"\"\"\n",
    "    Predict class of data, and append predictions to data frame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # drop old predictions before reclassifying (if they exist)\n",
    "        data_subset = data_subset.drop('predictions', axis = 1)\n",
    "        data_to_classify = data_subset.copy()\n",
    "    except:\n",
    "        data_to_classify = data_subset.copy()\n",
    "        pass\n",
    "    \n",
    "    # convert to numpy and classify\n",
    "    data_matrix = data_to_classify.as_matrix()\n",
    "    predictions = classifier.predict(data_matrix[::,1::])\n",
    "    \n",
    "    # append predictions to dataframe\n",
    "    data_to_classify['predictions'] = predictions\n",
    "    \n",
    "    # print statstics\n",
    "    counts = collections.Counter(predictions)\n",
    "    \n",
    "    for key in counts:\n",
    "        percent = (counts[key]/(len(predictions))* 100)\n",
    "        print(round(percent, 2), 'of data was classified as ', key)\n",
    "\n",
    "    return data_to_classify\n",
    "    \n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validation data from slave logs data set\n",
      "Validation set was classified as 1 99.95 % of the time\n",
      "Testing validation data from cliwoc data set:\n",
      "Validation set was classified as 1 0.0 % of the time\n"
     ]
    }
   ],
   "source": [
    "print('Testing validation data from slave logs data set')\n",
    "validation_test(classifier, validation_set_2, 1)\n",
    "\n",
    "print('Testing validation data from cliwoc data set:')\n",
    "validation_test(classifier, validation_set_1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.74 of data was classified as  0.0\n",
      "0.26 of data was classified as  1.0\n"
     ]
    }
   ],
   "source": [
    "unclassified_logs = append_predictions(classifier, unclassified_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_classifier = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classifier.fit(training_data[::,1::], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validation data from slave logs data set\n",
      "Validation set was classified as 1 100.0 % of the time\n",
      "Testing validation data from cliwoc data set:\n",
      "Validation set was classified as 1 35.71 % of the time\n"
     ]
    }
   ],
   "source": [
    "print('Testing validation data from slave logs data set')\n",
    "validation_test(new_classifier, validation_set_2, 1)\n",
    "\n",
    "print('Testing validation data from cliwoc data set:')\n",
    "validation_test(new_classifier, validation_set_1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.34 of data was classified as  0.0\n",
      "30.66 of data was classified as  1.0\n"
     ]
    }
   ],
   "source": [
    "unclassified_logs = predict_class(new_classifier, unclassified_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the below doesn't work...need to install graphviz but cannot access website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "startswith first arg must be str or a tuple of str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-a4068616a441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.pdf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Emma\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mgraph_from_dot_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdot_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Emma\\Anaconda3\\lib\\site-packages\\dot_parser.py\u001b[0m in \u001b[0;36mparse_dot_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[0mtop_graphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'utf-8'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: startswith first arg must be str or a tuple of str, not bytes"
     ]
    }
   ],
   "source": [
    "# export PDF with decision tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "import os\n",
    "import pydot \n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(new_classifier, out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"test.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
